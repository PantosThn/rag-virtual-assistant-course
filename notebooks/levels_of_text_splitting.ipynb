{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9663ad9-217c-46d3-befc-8ccabeba890b",
   "metadata": {},
   "source": [
    "# Levels Of Text Splitting\n",
    "\n",
    "In this tutorial we are reviewing the 4 Levels Of Text Splitting. This is an unofficial list put together for fun and educational purposes.\n",
    "\n",
    "Ever try to put a long piece of text into ChatGPT but it tells you it’s too long? Or you're trying to give your application better long term memory, but it’s still just not quite working.\n",
    "\n",
    "One of the most effective strategies to improve performance of your language model applications is to split your large data into smaller pieces. This is call splitting or chunking (we'll use these terms interchangeably). In the world of multi-modal, splitting also applies to images.\n",
    "\n",
    "We are going to cover a lot, but if you make it to the end, I guarantee you’ll have a solid grasp on chunking theory, strategies, and resources to learn more.\n",
    "\n",
    "**Levels Of Text Splitting**\n",
    "* **Level 1: [Character Splitting](#CharacterSplitting)** - Simple static character chunks of data\n",
    "* **Level 2: [Recursive Character Text Splitting](#RecursiveCharacterSplitting)** - Recursive chunking based on a list of separators\n",
    "* **Level 3: [Document Specific Splitting](#DocumentSpecific)** - Various chunking methods for different document types (PDF, Python, Markdown)\n",
    "* **Level 4: [Semantic Splitting](#SemanticChunking)** - Embedding walk based chunking\n",
    "\n",
    "**Notebook resources:**\n",
    "* [Video Overview]() - Walkthrough of this code with commentary\n",
    "* [ChunkViz.com](https://www.chunkviz.com/) - Visual representation of chunk splitting methods\n",
    "* [RAGAS](https://github.com/explodinggradients/ragas) - Retrieval evaluation framework\n",
    "\n",
    "**Evaluations**\n",
    "\n",
    "It's important to test your chunking strategies in retrieval evals. It doesn't matter how you chunk if the performance of your application isn't great.\n",
    "\n",
    "Eval Frameworks:\n",
    "\n",
    "* [LangChain Evals](https://python.langchain.com/docs/guides/evaluation/)\n",
    "* [Llama Index Evals](https://docs.llamaindex.ai/en/stable/module_guides/evaluating/root.html)\n",
    "* [RAGAS Evals](https://github.com/explodinggradients/ragas)\n",
    "\n",
    "\n",
    "**The Chunking Commandment:** Your goal is not to chunk for chunking sake, our goal is to get our data in a format where it can be retrieved for value later.\n",
    "\n",
    "## Level 1: Character Splitting <a id=\"CharacterSplitting\"></a>\n",
    "Character splitting is the most basic form of splitting up your text. It is the process of simply dividing your text into N-character sized chunks regardless of their content or form.\n",
    "\n",
    "This method isn't recommended for any applications - but it's a great starting point for us to understand the basics.\n",
    "\n",
    "* **Pros:** Easy & Simple\n",
    "* **Cons:** Very rigid and doesn't take into account the structure of your text\n",
    "\n",
    "Concepts to know:\n",
    "* **Chunk Size** - The number of characters you would like in your chunks. 50, 100, 100,000, etc.\n",
    "* **Chunk Overlap** - The amount you would like your sequential chunks to overlap. This is to try to avoid cutting a single piece of context into multiple pieces. This will create duplicate data across chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c96299fc-30f5-4edf-ac23-23a29f9c7282",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is the text I would like to chunk up. It is the example text for this exercise\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1cf67e-98d7-48bd-9867-f72be72e3f4a",
   "metadata": {},
   "source": [
    "Then let's split this text manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f11fb88f-17ed-44c2-b4de-a8a527fe63c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the text I would like to ch',\n",
       " 'unk up. It is the example text for ',\n",
       " 'this exercise']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list that will hold your chunks\n",
    "chunks = []\n",
    "\n",
    "chunk_size = 35 # Characters\n",
    "\n",
    "# Run through the a range with the length of your text and iterate every chunk_size you want\n",
    "for i in range(0, len(text), chunk_size):\n",
    "    chunk = text[i:i + chunk_size]\n",
    "    chunks.append(chunk)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140085b7-c6af-4003-923c-73feb1825965",
   "metadata": {},
   "source": [
    "When working with text in the language model world, we don't deal with raw strings. It is more common to work with documents. Documents are objects that hold the text you're concerned with, but also additional metadata which makes filtering and manipulation easier later.\n",
    "\n",
    "We could convert our list of strings into documents, but I'd rather start from scratch and create the docs.\n",
    "\n",
    "Let's load up LangChains `CharacterSplitter` to do this for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d85945f0-4a09-4bd9-bdb6-bafe03089053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5f3c0a-09c9-45a9-8f47-d28baf22b201",
   "metadata": {},
   "source": [
    "Then let's load up this text splitter. I need to specify `chunk overlap` and `separator` or else we'll get funk results. We'll get into those next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dcbeb8d-c5a0-4047-8250-967313c20935",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size = 35, chunk_overlap=0, separator='', strip_whitespace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae25bbe-d7d1-44da-820b-3cd34a1cfc67",
   "metadata": {},
   "source": [
    "Then we can actually split our text via `create_documents`. Note: `create_documents` expects a list of texts, so if you just have a string (like we do) you'll need to wrap it in `[]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afe4945b-ce08-49aa-a5dc-65a0e59922f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='This is the text I would like to ch'),\n",
       " Document(metadata={}, page_content='unk up. It is the example text for '),\n",
       " Document(metadata={}, page_content='this exercise')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331025f4-6ef4-4459-bcb6-df7824e78ce4",
   "metadata": {},
   "source": [
    "Notice how this time we have the same chunks, but they are in documents. These will play nicely with the rest of the LangChain world. Also notice how the trailing whitespace on the end of the 2nd chunk is missing. This is because LangChain removes it, see [this line](https://github.com/langchain-ai/langchain/blob/f36ef0739dbb548cabdb4453e6819fc3d826414f/libs/langchain/langchain/text_splitter.py#L167) for where they do it. You can avoid this with `strip_whitespace=False`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed0f193-4098-4fb1-a42f-7f96cd182188",
   "metadata": {},
   "source": [
    "**Chunk Overlap & Separators**\n",
    "\n",
    "**Chunk overlap** will blend together our chunks so that the tail of Chunk #1 will be the same thing and the head of Chunk #2 and so on and so forth.\n",
    "\n",
    "This time I'll load up my overlap with a value of 4, this means 4 characters of overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc66f496-7b0d-4b2a-a43d-e8f06d58c934",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size = 35, chunk_overlap=4, separator='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd5d7e36-b592-430e-9069-cc025c78d7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='This is the text I would like to ch'),\n",
       " Document(metadata={}, page_content='o chunk up. It is the example text'),\n",
       " Document(metadata={}, page_content='ext for this exercise')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd4aaa8-b90b-499e-b2d5-bc623b5bb751",
   "metadata": {},
   "source": [
    "Notice how we have the same chunks, but now there is overlap between 1 & 2 and 2 & 3. The 'o ch' on the tail of Chunk #1 matches the 'o ch' of the head of Chunk #2.\n",
    "\n",
    "I wanted a better way to visualize this, so I made [ChunkViz.com](www.chunkviz.com) to help show it. Here's what the same text looks like.\n",
    "\n",
    "**Separators** are character(s) sequences you would like to split on. Say you wanted to chunk your data at `ch`, you can specify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "814ce9aa-17c3-4205-b433-2eae612c2225",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size = 35, chunk_overlap=0, separator='ch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb759b1f-dab0-4f5e-a0c0-220374313da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='This is the text I would like to'),\n",
       " Document(metadata={}, page_content='unk up. It is the example text for this exercise')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c7ff4a-b6f0-4924-9a9c-2791dddc5b37",
   "metadata": {},
   "source": [
    "#### Llama Index\n",
    "\n",
    "[Llama Index](https://www.llamaindex.ai/) is a great choice for flexibility in the chunking and indexing process. They provide node relationships out of the box which can aid in retrieval later.\n",
    "\n",
    "Let's take a look at their sentence splitter. It is similar to the character splitter, but using its default settings, it'll split on sentences instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "624832d6-f3fd-45c4-b52e-bbc86e5e3cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e77494-a253-4414-b32d-2246fcb396ef",
   "metadata": {},
   "source": [
    "Load up your splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef9e6627-81a5-463d-9180-5ff2ff1d40f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = SentenceSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23214df1-064d-47ad-9058-f9a5f113b6cf",
   "metadata": {},
   "source": [
    "Load up your document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cafba29-b973-4b37-806f-6f4893eae02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"data/popular.txt\"]\n",
    ").load_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ceae08-1f5e-46bc-9d98-e95182fe8c3c",
   "metadata": {},
   "source": [
    "Create your nodes. Nodes are similar to documents but with more relationship data added to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52f9a2e2-3d56-4727-afce-c81c13089324",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec353aac-ea8f-421a-9895-0af8afdc08e0",
   "metadata": {},
   "source": [
    "Then let's take a look at one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "898a421f-54a0-45a4-8acb-c7087b6a883f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextNode(id_='a7c407b0-544c-437e-b087-8bffbd3c2acb', embedding=None, metadata={'file_path': 'data/popular.txt', 'file_name': 'popular.txt', 'file_type': 'text/plain', 'file_size': 43295, 'creation_date': '2025-05-18', 'last_modified_date': '2025-05-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='4d06bc0d-db85-4c68-8f04-f88226cef9ba', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': 'data/popular.txt', 'file_name': 'popular.txt', 'file_type': 'text/plain', 'file_size': 43295, 'creation_date': '2025-05-18', 'last_modified_date': '2025-05-18'}, hash='183fcc14103b24683d228fa3581151119ed0bbce9b806b38cfbd4c98944b88cd'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='75c7fabd-15c5-42ee-9be0-1a37d3f46f58', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='bd44b0c0745baddca30458110d0412318b00a3877b3e352db8452d5b6e944655')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"May 2001(This article was written as a kind of business plan for a\\nnew language.\\nSo it is missing (because it takes for granted) the most important\\nfeature of a good programming language: very powerful abstractions.)A friend of mine once told an eminent operating systems\\nexpert that he wanted to design a really good\\nprogramming language.  The expert told him that it would be a\\nwaste of time, that programming languages don't become popular\\nor unpopular based on their merits, and so no matter how\\ngood his language was, no one would use it.  At least, that\\nwas what had happened to the language he had designed.What does make a language popular?  Do popular\\nlanguages deserve their popularity?  Is it worth trying to\\ndefine a good programming language?\", mimetype='text/plain', start_char_idx=0, end_char_idx=755, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539aa1a2-67b8-4585-b0d3-306703ea856b",
   "metadata": {},
   "source": [
    "As you can see there is a lot more relationship data held within Llama Index's nodes. We'll talk about those later, I don't want to get ahead of ourselves\n",
    "\n",
    "Basic Character splitting is likely only useful for a few applications, maybe yours!\n",
    "\n",
    "## Level 2: Recursive Character Text Splitting\n",
    "<a id=\"RecursiveCharacterSplitting\"></a>\n",
    "Let's jump a level of complexity.\n",
    "\n",
    "The problem with Level #1 is that we don't take into account the structure of our document at all. We simply split by a fix number of characters.\n",
    "\n",
    "The Recursive Character Text Splitter helps with this. With it, we'll specify a series of separatators which will be used to split our docs.\n",
    "\n",
    "You can see the default separators for LangChain [here](https://github.com/langchain-ai/langchain/blob/9ef2feb6747f5a69d186bd623b569ad722829a5e/libs/langchain/langchain/text_splitter.py#L842). Let's take a look at them one by one.\n",
    "\n",
    "* \"\\n\\n\" - Double new line, or most commonly paragraph breaks\n",
    "* \"\\n\" - New lines\n",
    "* \" \" - Spaces\n",
    "* \"\" - Characters\n",
    "\n",
    "I'm not sure why a period (\".\") isn't included on the list, perhaps it is not universal enough? If you know, let me know.\n",
    "\n",
    "This is the swiss army knife of splitters and my first choice when mocking up a quick application. If you don't know which splitter to start with, this is a good first bet.\n",
    "\n",
    "Let's try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49f42bea-3d06-404d-9f8c-f15f7ff7591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6f79f4-769b-474b-8d7d-19cb48407cd6",
   "metadata": {},
   "source": [
    "Then let's load up a larger piece of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0772695d-0c5e-4e19-bb69-14e9bd7a15a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "One of the most important things I didn't understand about the world when I was a child is the degree to which the returns for performance are superlinear.\n",
    "\n",
    "Teachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true. If your product is only half as good as your competitor's, you don't get half as many customers. You get no customers, and you go out of business.\n",
    "\n",
    "It's obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the rich get richer. [1]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbb158c-6bbe-4f49-95df-a8b43965a566",
   "metadata": {},
   "source": [
    "Now let's make our text splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03ec54c4-bda6-4254-97dd-983775b1d729",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 65, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "887c7676-1e67-4084-94d3-59689eb399c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content=\"One of the most important things I didn't understand about the\"),\n",
       " Document(metadata={}, page_content='world when I was a child is the degree to which the returns for'),\n",
       " Document(metadata={}, page_content='performance are superlinear.'),\n",
       " Document(metadata={}, page_content='Teachers and coaches implicitly told us the returns were linear.'),\n",
       " Document(metadata={}, page_content='\"You get out,\" I heard a thousand times, \"what you put in.\" They'),\n",
       " Document(metadata={}, page_content='meant well, but this is rarely true. If your product is only'),\n",
       " Document(metadata={}, page_content=\"half as good as your competitor's, you don't get half as many\"),\n",
       " Document(metadata={}, page_content='customers. You get no customers, and you go out of business.'),\n",
       " Document(metadata={}, page_content=\"It's obviously true that the returns for performance are\"),\n",
       " Document(metadata={}, page_content='superlinear in business. Some think this is a flaw of'),\n",
       " Document(metadata={}, page_content='capitalism, and that if we changed the rules it would stop being'),\n",
       " Document(metadata={}, page_content='true. But superlinear returns for performance are a feature of'),\n",
       " Document(metadata={}, page_content=\"the world, not an artifact of rules we've invented. We see the\"),\n",
       " Document(metadata={}, page_content='same pattern in fame, power, military victories, knowledge, and'),\n",
       " Document(metadata={}, page_content='even benefit to humanity. In all of these, the rich get richer.'),\n",
       " Document(metadata={}, page_content='[1]')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa00043-1655-4113-bb28-f3a998d5713a",
   "metadata": {},
   "source": [
    "Notice how now there are more chunks that end with a period \".\". This is because those likely are the end of a paragraph and the splitter first looks for double new lines (paragraph break).\n",
    "\n",
    "Once paragraphs are split, then it looks at the chunk size, if a chunk is too big, then it'll split by the next separator. If the chunk is still too big, then it'll move onto the next one and so forth.\n",
    "\n",
    "For text of this size, let's split on something bigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6da8734e-47da-4a08-8459-9bf8bfed7fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content=\"One of the most important things I didn't understand about the world when I was a child is the degree to which the returns for performance are superlinear.\"),\n",
       " Document(metadata={}, page_content='Teachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true. If your product is only half as good as your competitor\\'s, you don\\'t get half as many customers. You get no customers, and you go out of business.'),\n",
       " Document(metadata={}, page_content=\"It's obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the rich get richer. [1]\")]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 450, chunk_overlap=0)\n",
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f32a73-0c8a-498c-a3a1-3e7dba4658c9",
   "metadata": {},
   "source": [
    "## Level 3: Document Specific Splitting <a id=\"DocumentSpecific\"></a>\n",
    "\n",
    "Stepping up our levels ladder, let's start to handle document types other than normal prose in a .txt. What if you have pictures? or a PDF? or code snippets?\n",
    "\n",
    "Our first two levels wouldn't work great for this so we'll need to find a different tactic.\n",
    "\n",
    "This level is all about making your chunking strategy fit your different data formats. Let's run through a bunch of examples of this in action\n",
    "\n",
    "The Markdown, Python, and JS splitters will basically be similar to Recursive Character, but with different separators.\n",
    "\n",
    "See all of LangChains document splitters [here](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/code_splitter) and Llama Index ([HTML](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules.html#htmlnodeparser), [JSON](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules.html#jsonnodeparser), [Markdown](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules.html#markdownnodeparser))\n",
    "\n",
    "### Markdown\n",
    "\n",
    "You can see the separators [here](https://github.com/langchain-ai/langchain/blob/9ef2feb6747f5a69d186bd623b569ad722829a5e/libs/langchain/langchain/text_splitter.py#L1175).\n",
    "\n",
    "Separators:\n",
    "* `\\n#{1,6}` - Split by new lines followed by a header (H1 through H6)\n",
    "* ```` ```\\n ```` - Code blocks\n",
    "* `\\n\\\\*\\\\*\\\\*+\\n` - Horizontal Lines\n",
    "* `\\n---+\\n` - Horizontal Lines\n",
    "* `\\n___+\\n` - Horizontal Lines\n",
    "* `\\n\\n` Double new lines\n",
    "* `\\n` - New line\n",
    "* `\" \"` - Spaces\n",
    "* `\"\"` - Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "298fe868-0872-4fa9-9146-fa33e9dd5706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1d390ed-d046-44f9-a492-9760141f7982",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = MarkdownTextSplitter(chunk_size = 40, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ba14168-451b-4e9c-b1d0-d1eac6996ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_text = \"\"\"\n",
    "# Fun in California\n",
    "\n",
    "## Driving\n",
    "\n",
    "Try driving on the 1 down to San Diego\n",
    "\n",
    "### Food\n",
    "\n",
    "Make sure to eat a burrito while you're there\n",
    "\n",
    "## Hiking\n",
    "\n",
    "Go to Yosemite\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15dcf8de-551a-4477-8e68-57c4c50ddbc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='# Fun in California\\n\\n## Driving'),\n",
       " Document(metadata={}, page_content='Try driving on the 1 down to San Diego'),\n",
       " Document(metadata={}, page_content='### Food'),\n",
       " Document(metadata={}, page_content=\"Make sure to eat a burrito while you're\"),\n",
       " Document(metadata={}, page_content='there'),\n",
       " Document(metadata={}, page_content='## Hiking\\n\\nGo to Yosemite')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter.create_documents([markdown_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56591620-ef0c-41c2-b539-35ad676ed20f",
   "metadata": {},
   "source": [
    "Notice how the splits gravitate towards markdown sections. However, it's still not perfect. Check out how there is a chunk with just \"there\" in it. You'll run into this at low-sized chunks.\n",
    "\n",
    "### Python\n",
    "\n",
    "See the python splitters [here](https://github.com/langchain-ai/langchain/blob/9ef2feb6747f5a69d186bd623b569ad722829a5e/libs/langchain/langchain/text_splitter.py#L1069)\n",
    "\n",
    "* `\\nclass` - Classes first\n",
    "* `\\ndef` - Functions next\n",
    "* `\\n\\tdef` - Indented functions\n",
    "* `\\n\\n` - Double New lines\n",
    "* `\\n` - New Lines\n",
    "* `\" \"` - Spaces\n",
    "* `\"\"` - Characters\n",
    "\n",
    "\n",
    "Let's load up our splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66edcde5-1e96-4b61-8636-8129d31d7850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import PythonCodeTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2afa8f93-6b07-484f-86ff-9836f5a5fae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_text = \"\"\"\n",
    "class Person:\n",
    "  def __init__(self, name, age):\n",
    "    self.name = name\n",
    "    self.age = age\n",
    "\n",
    "p1 = Person(\"John\", 36)\n",
    "\n",
    "for i in range(10):\n",
    "    print (i)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e8fcc85-714d-4b5c-a5ce-a3f30cfb447b",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_splitter = PythonCodeTextSplitter(chunk_size=100, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7b6dd89-6bb9-496a-a85d-3f1871ff9cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='class Person:\\n  def __init__(self, name, age):\\n    self.name = name\\n    self.age = age'),\n",
       " Document(metadata={}, page_content='p1 = Person(\"John\", 36)\\n\\nfor i in range(10):\\n    print (i)')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_splitter.create_documents([python_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c004f19-9e67-451e-abdd-b103acce2996",
   "metadata": {},
   "source": [
    "Check out how the class stays together in a single document (good), then the rest of the code is in a second document (ok).\n",
    "\n",
    "I needed to play with the chunk size to get a clean result like that. You'll likely need to do the same for yours which is why using evaluations to determine optimal chunk sizes is crucial.\n",
    "\n",
    "### JS\n",
    "\n",
    "Very similar to python. See the separators [here](https://github.com/langchain-ai/langchain/blob/9ef2feb6747f5a69d186bd623b569ad722829a5e/libs/langchain/langchain/text_splitter.py#L983).\n",
    "\n",
    "Separators:\n",
    "* `\\nfunction` - Indicates the beginning of a function declaration\n",
    "* `\\nconst` - Used for declaring constant variables\n",
    "* `\\nlet` - Used for declaring block-scoped variables\n",
    "* `\\nvar` - Used for declaring a variable\n",
    "* `\\nclass` - Indicates the start of a class definition\n",
    "* `\\nif` - Indicates the beginning of an if statement\n",
    "* `\\nfor` - Used for for-loops\n",
    "* `\\nwhile` - Used for while-loops\n",
    "* `\\nswitch` - Used for switch statements\n",
    "* `\\ncase` - Used within switch statements\n",
    "* `\\ndefault` - Also used within switch statements\n",
    "* `\\n\\n` - Indicates a larger separation in text or code\n",
    "* `\\n` - Separates lines of code or text\n",
    "* `\" \"` - Separates words or tokens in the code\n",
    "* `\"\"` - Makes every character a separate element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5225b66-4d79-455b-92a1-841fa23ccc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d75090fa-4d22-4348-8452-eb50eafa784d",
   "metadata": {},
   "outputs": [],
   "source": [
    "javascript_text = \"\"\"\n",
    "// Function is called, the return value will end up in x\n",
    "let x = myFunction(4, 3);\n",
    "\n",
    "function myFunction(a, b) {\n",
    "// Function returns the product of a and b\n",
    "  return a * b;\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "909fde28-43ba-4f07-b9ae-04c21db04055",
   "metadata": {},
   "outputs": [],
   "source": [
    "js_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.JS, chunk_size=65, chunk_overlap=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b70d936-bc31-4ecc-b190-6dd8fffdacb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='// Function is called, the return value will end up in x'),\n",
       " Document(metadata={}, page_content='let x = myFunction(4, 3);'),\n",
       " Document(metadata={}, page_content='function myFunction(a, b) {'),\n",
       " Document(metadata={}, page_content='// Function returns the product of a and b\\n  return a * b;\\n}')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "js_splitter.create_documents([javascript_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fab2014-e705-4ade-87ea-d967a9c01593",
   "metadata": {},
   "source": [
    "### PDFs w/ tables\n",
    "\n",
    "Ok now things will get a bit spicier.\n",
    "\n",
    "PDFs are an extremely common data type for language model work. Often they'll contain tables that contain information.\n",
    "\n",
    "This could be financial data, studies, academic papers, etc.\n",
    "\n",
    "Trying to split tables by a character based separator isn't reliable. We need to try out a different method. For a deep dive on this I recommend checking out [Lance Martin's](https://twitter.com/RLanceMartin) [tutorial](https://twitter.com/RLanceMartin/status/1721942636364456336) w/ LangChain.\n",
    "\n",
    "I'll be going through a text based methods. [Mayo](https://twitter.com/mayowaoshin) has also outlined a GPT-4V method which tries to pulls tables via vision rather than text. You can check out [here](https://twitter.com/mayowaoshin/status/1727399231734886633).\n",
    "\n",
    "A very convenient way to do this is with [Unstructured](https://unstructured.io/), a library dedicated to making your data LLM ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffd078d0-5651-4ab0-b299-b2ed5a4f7cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.staging.base import elements_to_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b174a72b-0d43-4440-9ef8-f5f3aed0c651",
   "metadata": {},
   "source": [
    "Let's load up our PDF and then parition it. This is a PDF from a [Salesforce earning report](https://investor.salesforce.com/financials/default.aspx)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ae9042d-be6f-411a-8835-bda30ffa0805",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot set gray non-stroke color because /'P274' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P276' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P278' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P279' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P281' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P282' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P283' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P284' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P286' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P287' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P288' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P289' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P290' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P291' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P292' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P293' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P294' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P295' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P296' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P297' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P298' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P299' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P300' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P301' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P302' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P303' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P304' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P305' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P306' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P307' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P308' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P309' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P310' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P311' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P313' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P314' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P319' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P320' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P321' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P322' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P324' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P325' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P326' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P327' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P328' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P329' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P330' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P331' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P332' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P333' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P334' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P335' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P336' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P337' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P274' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P276' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P278' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P279' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P281' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P282' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P283' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P284' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P286' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P287' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P288' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P289' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P290' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P291' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P292' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P293' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P294' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P295' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P296' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P297' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P298' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P299' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P300' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P301' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P302' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P303' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P304' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P305' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P306' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P307' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P308' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P309' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P310' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P311' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P313' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P314' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P319' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P320' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P321' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P322' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P324' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P325' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P326' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P327' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P328' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P329' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P330' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P331' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P332' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P333' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P334' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P335' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P336' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P337' is an invalid float value\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf6286c6ddff4885b3ed5beb2b9799a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b71be6ffbb49b2b0a9d1b04bbaa839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/115M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9a36ce646c4c839cfb3d0f7591c2a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/46.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename = \"data/Note_on_the_Greek_economy_14_February_2025.pdf\"\n",
    "\n",
    "# Extracts the elements from the PDF\n",
    "elements = partition_pdf(\n",
    "    filename=filename,\n",
    "\n",
    "    # Unstructured Helpers\n",
    "    strategy=\"hi_res\", \n",
    "    infer_table_structure=True, \n",
    "    model_name=\"yolox\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65ad413-80f9-43da-8fb5-3a32373c3686",
   "metadata": {},
   "source": [
    "Let's look at our elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a081974-002c-4060-9d0c-2d4a5f270044",
   "metadata": {},
   "source": [
    "These are just unstructured objects, we could look at them all but I want to look at the table it parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c5460db-689d-4e7a-a5bc-a10477c4a61e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Employment increased by 0.8% y-o-y in December 2024, while the monthly unemployment rate (sa) decreased to 9.4%.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements[20].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b8378604-a4da-421e-93c6-cae8bf0a808a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'unstructured.documents.elements.Image'>   \n",
      "<class 'unstructured.documents.elements.Image'> \n",
      "<class 'unstructured.documents.elements.Title'> NOTE ON THE GREEK ECONOMY February 14, 2025\n",
      "<class 'unstructured.documents.elements.NarrativeText'> Economic Analysis and Research Department\n",
      "<class 'unstructured.documents.elements.NarrativeText'> Recent Economic Developments: an overview\n",
      "<class 'unstructured.documents.elements.NarrativeText'> Economic activity continued to expand at a satisfactory pace\n",
      "<class 'unstructured.documents.elements.NarrativeText'> Looking ahead, according to the latest BoG projections, grow\n",
      "<class 'unstructured.documents.elements.Text'> ------------------------------------------------------------\n",
      "<class 'unstructured.documents.elements.Title'> Latest economic information - available in the last three we\n",
      "<class 'unstructured.documents.elements.Title'> Economic Activity\n"
     ]
    }
   ],
   "source": [
    "from unstructured.documents.elements import Table, NarrativeText, Title\n",
    "\n",
    "# Option 1: Safely extract tables\n",
    "tables = [el for el in elements if isinstance(el, Table)]\n",
    "\n",
    "# Option 2: All text-based elements\n",
    "text_elements = [el for el in elements if hasattr(el, 'text') and isinstance(el.text, str)]\n",
    "\n",
    "# Option 3: Print types to explore what you're working with\n",
    "for el in elements[:10]:\n",
    "    print(type(el), getattr(el, \"text\", \"\")[:60])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3d2470b5-481c-4842-b2d0-458e174cb658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Table 1 ---\n",
      "SUMMARY OF ECONOMIC DEVELOPMENTS AND OUTLOOK..............ccccccssesseeeeeeteeerees 4 BACKGROUND INFORMATION .............cccccccsscccsceseseceeseesssesecsessecesecseceeseseeseeeessesseeeseseseeseesags 8 1. ECONOMIC ACTIVITY 0.00... ccc cccccccecsseccessesseeececseseessseesessessesessesseseseseeseaseseesseseseesessesesens 8 2. PRICES AND REAL ESTATE MARKET ...............c:ccccccesssscssescsscssescsesseseseessesessecsesesseseesesseseesasegs 13 3. LABOUR MARKET AND COSTS ...........cccccceccescsseceessecsceececeesecesecseseessecesecsnsesseseeseseeseesanegs 17 4. EXTERNAL BALANCES, COMPETITIVENESS. ...............0:ccsccccsscssessessesseseseeseseeseessseseeseesenee 21 5. FISCAL DEVELOPMENTS..............c:cccccccsscssecsssseesceessesscsessscscsessesecseseessesessessssesseseeseseeseesaaees 27 6. MONEY AND CREDIT ...........:.ccccecsccccccecsecesscsesssesceessesscsecsecsesessesecseseessssessesssseseseeseaseseeseases 32 7. FINANCIAL MARKET DEVELOPMENT G.............ccccccccsscsscsscsssscses\n",
      "\n",
      "--- Table 2 ---\n",
      "2022 | 2023 | 2024 2024 2024 2025 Qi Q2 Q3 Q4 |} June July Aug Sep Oct Nov Dec | Jan Feb GDP, % y-o-y 5.7 23 ws 22 23 24 wa - - - - - - - - - Exports, % y-o-y 6.6 19 ws 530 17 33 wa - - - - - - - - - Industrial production, % y-o-y 2.4 2.3 5.3 3.6 9.6 5.7 21 9.7 10.3 3.7 25 -2.6 3.3 5.8 Retail sales volume, % y-o-y 3.3 -3.3 o 47 3.2 -2.8 we 6.0 25 S51 -06 -15 11 PMI (S0=no change) $1.8 51.6 53.6 55.8 54.7 521 518 540 532 529 503 512 509 53.2 52.8 ESI (average=100) 104.8 107.6 107.6 107.3 110.4 107.6 106.4 109.8 106.2 106.0 110.2 107.7 106.8 106.4 108.6 HICP, % y-o-y 9.3 42 3.0 3.2 27 3.1 3.0 2.5 3.0 3.2 3.1 3.1 3.0 2.9 3.1 Total employment, % y-o-y 5.4 13 ws 18 22 16 wa 27° 10 13 23 18 #34 08 Unemployment rate, % 12.4 111 o 12.1 9.8 9.0 we 9.4 97 9.5 9.4 9.7 9.5 9.4 Current Account, bn. -21.2 -13.9 o 3.8 -4.5 0.3 we 0.3 0.2 0.5 03 O04 -3.2 (%ofGDP) — 10.2% © -6.2% Gen. Gov. primary balance 00 9 2a w 06 15 37) - - - - ee (% of GDP - Q cumulatively) Public Debt . 1770 1639 .. 155.5 1559 1\n",
      "\n",
      "--- Table 3 ---\n",
      "(year-on-year changes) 2023 2024 2025' 2026' 2027' Real GDP 23 23 25 23 2.0 Private consumption 1.8 21 21 21 21 Government consumption 2.6 11 01 0.1 0.7 Gross fixed capital formation 6.6 9.5 8.2 73 0.9 Exports (goods and services) 1.9 17 37 3.8 3.9 Imports (goods and services) 09 37 3.8 3.9 2.9 HICP (non-SA) 4.2 3.0 25 2.2 25 HICP excluding food & energy (non-SA) 5.3 3.5 3.1 24 2.2 Total employment (NA data) 1.2 14 13 1.2 11 Unemployment rate (% of labour force) 11.1 10.6 9.8 9.1 8.5 Current account (% of nom.GDP) 6.2 6.5 -6.0 5.2 5.2\n",
      "\n",
      "--- Table 4 ---\n",
      "2024 %y-0-V qi a2 Q3 1.GDP 5.7 23 21 2.2 2.3 24 -Private consumption 8.6 1.8 1.4 1.6 1.8 2.1 -Gov. expenditure 0.1 2.6 4.0 6.8 4.2 -1.4 -Gross fixed capital formation 16.4 6.6 0.6 2.6 3.7 0.3 -Exports 6.6 1.9 0.9 5.3 1.7 3.3 -Imports 11.0 0.9 2.4 3.7 8.7 4.2 2. Gross Value Added 5.3 2.2 2.1 1.1 2.4 2.3 “Services 5.5 3.3 2.0 0.7 1.2 1.9 3. Private sector savings* 8.4 8.0 8.0 7.9 7.2 6.9 4. Real disposable income 0.4 3.3 3.7 1.0 1.8 3.0\n",
      "\n",
      "--- Table 5 ---\n",
      "2022 2023 2024 1. ESI (average=100) 104.9 107.2 107.6 107.0 1047 -Consumer confidence 50.7 40.0 -46.0 -46.3 -47.2 2. PMI (50=no change) 518 516 536 547 557 3. Industrial Production, % y-o-y 24 2300 53 103 21 -Manufacturing Production, % y-o-y 46 42 38 54 28 4, Turnover of enterprises, % y-0-y 360-28 1597 5. Building permits, % y-o-y 22 15.9 96 75.6 6. Real VAT revenues, % y-0-y 12.2. 55 32 72 7. Retail sales volume, % y-o-y 3300-33 93-95 8. New car registrations, % y-o-y 67 165 34 94 185 9. Tourist arrivals, % y-o-y 96.0 © 208 16.0 26.0 10. Travel receipts, % y-o-y 683 16.5 283 23.9 108.2 44,7 56.9 -06 108.2 -41.7 55.2 12.3 12.2 16.1 27.3 13.4 65 28.9 13.9 26.7 2025 Jan 110.2 109.8 106.2 106.0 110.2 107.7 106.8 1064 107.6 108.6 438 -427 -43.9 -48.1 -513 -503 -47.3 -445 -46.0 -43.4 54.9 54.0 53.2 52.9 50.3 51.2 50.9 53.2 53.6 52.8 6.8 9.7 10.3 3.7 25 -2.6 3.3 5.8 5.3 46 5.5 97 3.9 5.4 -2.5 15 3.6 3.8 15 27 12.7 29 19 42 3.4 46 3.8 120 -29 -13.9 23.5 46.7 14.7 6.9 5.6 10.5 13 10.8 12.1 1\n",
      "\n",
      "--- Table 6 ---\n",
      "% y-o-y, nsa data 2023 2024 2024 2025 May Jun Jul Aug Sep Oct Nov_—Dec Jan 1. HICP Headline 42 3.0 24 25 3.0 32 31 31 3.0 29 31 - Energy “13.4 14 18-24 14 23 08 16 23 0.7 26 - Unprocessed food a4 34 22 1700-12 20 44 15 17 0.0 08 - Processed food 93 25 25 27 25 24 23 08 0.0 03 03 font) Core (HICP excl. energy and 53 3.6 28 3.4 37 37 3.6 44 45 44 44 - Non-energy industrial goods 6A 17 15 41 09 14 18 16 24 17 14 - Services 45 44 33 4a 5.0 aq 44 56 56 5.6 5.6 3. PPI - Domestic market 65 24 37-24-05 08 A022 0.1 04 4, Imports Price Index “123 20 28 4a 160° «-43 1065528 0.6\n",
      "\n",
      "--- Table 7 ---\n",
      "2021 2022 2023 2024 %y-o-y Pm Qi Q2 Q3 1. Residential property -Apartment prices 7.6 11.9 13.8 15.2 12.6 10.0 15.6 14.8 12.7 12.5 106 94 7.8 - Residential Investment 31.8 57.8 24.7 57.1 0.7 -10.3 61.6 52.8 29.2 -18.7 -13.7 6.9 7.2 2.Commercial property - Prime office prices 1.7 36 60 70 49 4.2 - - - - - - - -Prime retail prices 25 62 7.0 7.0 6.9 7.8 - - - - - - - - Office rents 3.9 30 62 65 58 2.2 - - - - - - - -Retail rents 11 44 59 60 5.7 6.2 - - - - - - -\n",
      "\n",
      "--- Table 8 ---\n",
      "2023 2024 2024 2025 1. Labour Force Survey - Total employment (% y-0-y) 13 18 2.2 1.6 os 23 18 34 08 19 - Employees (% y-o-y) 04 12 1.6 2.6 os a . os os 18 - Self-employed (% y-0-y) 2.0 34 20 09 os a . os os 14 - Unemployment rate\" 1110012198 9.0 os 94 97 95 94 - Long-term unemployed (as % of unemployed) 56.0 520 532 565 2. ERGANI Information System - Net dependent employment flows in the private 116.6 56.2 283.0 365-2324 «3.6 131.5 86.7 = 14.2, 70.3, sector (thousands) ~ Share of part-time and intermittent jobs (% new 486 471 430 512 524 524 538 512 S17 48.0 hirings) 3. Registered unemployed (DYPA) (%y-0-y) 45° 50 64 75 62 68 71 61 57 59 4. Employment Expectations Index 109.4 1149 1194 1133 1113 1122 1114 1094 1130 114.7 1101 5. Labour Costs - Compensation per employee (% y-o-y) 37 TZ 8.4 73 os a . os os 7.6 - Labour productivity (% y-o-y) 1a 08 13 13 os a . os os 11 - Unit labour cost (% y-o-y) 25 63 7.0 6.0 os a . os os 6.4\n",
      "\n",
      "--- Table 9 ---\n",
      "2021 2022 2023 2024 Sep Oct Nov ytd Current Account, bn (%GDP) -12.3 (6.6%) -21.2(-10.2%) -13.9 (-6.2%) 0.3 0.4 3.2 “11.5 Goods balance, bn (%GDP) -26.7 (14.5%) -39.6 (-19.0%) -33.0 (14.7%) 3.1 3.2 3.2 32.5 Exports of goods (% y-o-y) 36.1 36.7 6.9 5.0 -10.1 5.8 3.6 - Exports of non-fuel goods (% y-o-y) 27.7 24.2 1.9 10.0 3.8 2.5 0.1 Imports of goods (% y-o-y) 39.2 413 -11.0 17 18 2.3 15 -Imports of non-fuel goods (% y-o-y) 31.0 25.0 1.6 2.4 11.4 3.5 4.2 Real trade in goods flows (% y-o-y) Real exports of goods (% y-0-y) 17.8 4.9 3.0 4.1 5.0 -2.0 -Real exports ofnon-fuel goods (% y-0-y) 21.0 77 5.7 8.2 14 3.1 Real imports of goods (% y-o-y) 23.9 17.5 33 6.2 14.9 3.8 -Real imports of non-fuel goods (% y-o-y) 27.5 16.8 2.8 17 13.0 3.2 Services balance, bn (%GDP) 12.8(7.0%) 19.4(9.3%) 21.8 (9.7%) 3.6 23 07 22.3 Exports of services (% y-o-y) 54.4 36.2 2.7 2.7 8.4 9.0 4.8 -Travel receipts (% y-o-y) 143.2 68.3 16.5 6.9 19.7 44.7 4.9 -Transportation receipts (% y-o-y) 35.6 25.1 -10.4 75 5.7 5.\n",
      "\n",
      "--- Table 10 ---\n",
      "2021 2022 2023 2024 Sep Oct Nov y-t-d - Structural funds 2391 2318 1256 0.0 0.0 0.0 0.0 -Farmers' subsidies 2213 1963 2493 72.8 0.7 9.2 1870.4 -NGEU ° Recovery and Resilience Facility (RRF)-grants * 2310 1718 3405 aa 999 aa 1157 ° Recovery and Resilience Facility (RRF)-loans 1655 1845 3793 we we we 2327\n"
     ]
    }
   ],
   "source": [
    "for i, table in enumerate(tables[:10]):\n",
    "    print(f\"\\n--- Table {i+1} ---\")\n",
    "    print(table.text[:1000])  # Show first 1000 characters of the table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae427f0a-864d-4a9c-b0a3-362a7b383f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Image ---\n",
      "  \n",
      "\n",
      "--- Image ---\n",
      "\n",
      "\n",
      "--- Title ---\n",
      "NOTE ON THE GREEK ECONOMY February 14, 2025\n",
      "\n",
      "--- NarrativeText ---\n",
      "Economic Analysis and Research Department\n",
      "\n",
      "--- NarrativeText ---\n",
      "Recent Economic Developments: an overview\n",
      "\n",
      "--- NarrativeText ---\n",
      "Economic activity continued to expand at a satisfactory pace in 2024:Q3 (2.4% y-o-y), outperforming the euro area average. HICP inflation came down fast from its 2022 peak due to falling energy prices in 2023, but it remained relatively elevated at 3.0% in 2024 due to persistent services inflation. \n",
      "\n",
      "--- NarrativeText ---\n",
      "Looking ahead, according to the latest BoG projections, growth is expected to peak in 2025 and converge thereafter towards potential growth. Growth will be mainly driven by private consumption and investment supported by available European resources. Inflation is expected to further decelerate in 20\n",
      "\n",
      "--- Text ---\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Title ---\n",
      "Latest economic information - available in the last three weeks\n",
      "\n",
      "--- Title ---\n",
      "Economic Activity\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from unstructured.documents.elements import Image as UnstructuredImage, NarrativeText\n",
    "\n",
    "for i, el in enumerate(elements[:10]):\n",
    "    print(f\"--- {type(el).__name__} ---\")\n",
    "\n",
    "    if hasattr(el, \"text\"):\n",
    "        print(el.text[:300])\n",
    "    elif isinstance(el, UnstructuredImage) and el.image_path:\n",
    "        print(f\"[Displaying image {i}]\")\n",
    "        display(IPyImage(filename=el.image_path))\n",
    "        # Optionally: look ahead for possible caption\n",
    "        if i + 1 < len(elements) and isinstance(elements[i + 1], NarrativeText):\n",
    "            print(\"Caption (approx.):\", elements[i + 1].text[:200])\n",
    "    else:\n",
    "        print(\"[No text content]\")\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dee2ed53-96c8-4cf3-89d9-681ff5d4552f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install \"unstructured[all-docs]\"\n",
    "from typing import Any\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from unstructured.partition.pdf import partition_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9240942-2702-45ae-8333-8ac9c3e10343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get elements\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=filepath,\n",
    "    \n",
    "    # Using pdf format to find embedded image blocks\n",
    "    extract_images_in_pdf=True,\n",
    "    \n",
    "    # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n",
    "    # Titles are any sub-section of the document\n",
    "    infer_table_structure=True,\n",
    "    \n",
    "    # Post processing to aggregate text once we have the title\n",
    "    chunking_strategy=\"by_title\",\n",
    "    # Chunking params to aggregate text blocks\n",
    "    # Attempt to create a new chunk 3800 chars\n",
    "    # Attempt to keep chunks > 2000 chars\n",
    "    # Hard max on chunks\n",
    "    max_characters=4000,\n",
    "    new_after_n_chars=3800,\n",
    "    combine_text_under_n_chars=2000,\n",
    "    image_output_dir_path=\"static/pdfImages/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c45aab5-5f51-4d67-9ea2-17c5489a5c2a",
   "metadata": {},
   "source": [
    "If you head over to `static/pdfImages/` and check out the images that were parsed.\n",
    "\n",
    "But the images don't do anything sitting in a folder, we need to do something with them! Though a bit outside the scope of chunking, let's talk about how to work with these.\n",
    "\n",
    "The common tactics will either use a multi-modal model to generate summaries of the images or use the image itself for your task. Others get embeddings of images (like [CLIP](https://openai.com/research/clip)).\n",
    "\n",
    "Let's generate summaries so you'll be inspired to take this to the next step. We'll use GPT-4V. Check out other models [here](https://platform.openai.com/docs/model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba7cf0e-ec8e-4115-be32-e49aaf5adccc",
   "metadata": {},
   "source": [
    "## Level 4: Semantic Chunking <a id=\"SemanticChunking\"></a>\n",
    "\n",
    "- **Definition**: Splitting a document into chunks based on meaning or topic rather than length or layout.\n",
    "\n",
    "- **Goal**: Make each chunk a self-contained idea or section so it's more useful for search and question-answering.\n",
    "\n",
    "- **How it's different**:\n",
    "  - Semantic: \"Split here because this section starts talking about inflation.\"\n",
    "  - Non semantic: \"Split every 500 characters, no matter what it says.\"\n",
    "\n",
    "- **Methods used**:\n",
    "  - Text embeddings + clustering\n",
    "  - Topic modeling (e.g., LDA)\n",
    "  - Sentence similarity\n",
    "  - Section headers or title detection\n",
    "\n",
    "- **Where it's used**:\n",
    "  - RAG pipelines (LLMs retrieve meaningful chunks)\n",
    "  - Document summarization\n",
    "  - Intelligent document search\n",
    "\n",
    "- **Benefits**:\n",
    "  - Better answers from LLMs\n",
    "  - Reduces irrelevant or partial content\n",
    "  - Improves retrieval precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0125d83-4b05-4be3-beb6-8171bbdaa268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Input is a list of elements (e.g., from unstructured or PDF parser)\n",
    "elements = partition_pdf(...)\n",
    "\n",
    "chunks = []\n",
    "current_chunk = {\n",
    "    \"title\": \"Untitled\",\n",
    "    \"content\": []\n",
    "}\n",
    "\n",
    "for el in elements:\n",
    "    if is_title(el):\n",
    "        # Save the previous chunk if it has content\n",
    "        if current_chunk[\"content\"]:\n",
    "            chunks.append(current_chunk)\n",
    "        \n",
    "        # Start a new chunk\n",
    "        current_chunk = {\n",
    "            \"title\": el.text.strip(),\n",
    "            \"content\": []\n",
    "        }\n",
    "    elif is_text(el):\n",
    "        current_chunk[\"content\"].append(el.text.strip())\n",
    "\n",
    "# Step 3: Save the last chunk\n",
    "if current_chunk[\"content\"]:\n",
    "    chunks.append(current_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f23d163-a51e-422a-ac04-743f74bc145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "  {\n",
    "    \"title\": \"Executive Summary\",\n",
    "    \"content\": [\n",
    "      \"The Greek economy grew by 2.4% in Q3...\",\n",
    "      \"Inflation fell from 9.2% to 3.1%...\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"title\": \"Macroeconomic Outlook\",\n",
    "    \"content\": [\n",
    "      \"Growth is expected to slow in 2025...\",\n",
    "      \"Private investment remains strong...\"\n",
    "    ]\n",
    "  }\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
